{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример состязательного автокодировщика, обучающегося порождать рукописные цифры из датасета MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(data,batch_n):# последовательно выдает мини-батчи данных в случайном порядке\n",
    "    inds = range(data.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "    for i in rande(data.shape[0] / batch_n):\n",
    "        ii = inds[i*batch_n:(i+1)*batch]\n",
    "        yield data[ii, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initializer(size): # задает инициализацию весов для ReLU-нейронов по Хе\n",
    "    return tf.random_normal_initializer(mean=0.0, stddev=np.sqrt(1./size),seed=None,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вспомогательная функция, которая задает стандартный линейный слой с матрицей весов, инициализирующейся init_fn,\n",
    "# и вектором свободных членов, инициализирующимся константами\n",
    "def linear_layer(tensor,input_size,out_size,init_fn=he_initializer,): \n",
    "    W = tf.get_variable('W', shape=[input_size, out_size], initializer=init_fn(input_size))\n",
    "    b = tf.get_variable('b', shape=[out_size], initializer=tf.constant_initializer(0.1))\n",
    "    return tf.add(tf.matmul(tensor,W),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выдает векторы случайных нормально распределенных чисел, пропущенные через гиперболический тангенс\n",
    "# эта функция пригодится для порождения мини-батчей случайных векторов для генератора\n",
    "def sample_prior(loc=0.,scale=1.,size=(64,10)):\n",
    "    return np.tanh(np.ramdom.normal(loc=loc,scale=scale,size=size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAE(object):\n",
    "    def __init__(self,batch_size=64, input_space=28*28,latent_space=10, p=3., middle_layers=None,\\\n",
    "                 activation_fn=tf.nn.tanh, learning_rate=0.001, l2_lambda = 0.001,\\\n",
    "                 initializer_fn=he_initializer):\n",
    "        self.batch_size = batch_size\n",
    "        self.input_space = input_space\n",
    "        self.latent_space = latent_space\n",
    "        self.p = p\n",
    "        self.middle_layers = [1024,1024]\n",
    "        self.activation_fn = activation_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initializer_fn = initializer_fn\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(tf.float32,[None, input_space])\n",
    "        self.z_tensor = tf.placeholder(tf.float32,[None,latent_space])\n",
    "        \n",
    "        #определим структуру трех составных часте ААЕ\n",
    "        def _encoder(self):\n",
    "            sizes = [self.input_space] + self.middle_layers + [self.latent_space]\n",
    "            self.encoder_layers = [self.input_x]\n",
    "            for i in range(len(sizes)-1):\n",
    "                with tf.variable_scope('layers-%s'% i):\n",
    "                    linear = linear_layer(self.encoder_layers[-1],sizes[i],sizes[i+1])\n",
    "                    self.encoder_layers.append(self.activation_fn(linear))\n",
    "            #return encoder_layers\n",
    "        \n",
    "        def _decoder(self):\n",
    "            sizes = [self.latent_space]+ self.middle_layers[::-1]\n",
    "            decoder_layers = [tensor]\n",
    "            for i in range (len(sizes)-1):\n",
    "                with tf.variable_scope('layers-%s' % i):\n",
    "                    linear = linear_layer(decoder_layers[-1],sizes[i],sizes[i+1])\n",
    "                    decoder_layers.append(self.activation_fn(linear))\n",
    "            with tf.variable_scope('output-layer'):\n",
    "                linear = linear_layer(decoder_layers[-1],sizes[-1], self.input_space)\n",
    "                decoder_layers.append(linear)\n",
    "            return decoder_layers\n",
    "            \n",
    "        def _discriminator(self,tensor,sizes):\n",
    "            sizes = [self.latent_space]+sizes + [1]\n",
    "            disc_layers = [tensor]\n",
    "            for i in range(len(sizes)-1):\n",
    "                with tf.variable_scope('layers-%s' % i):\n",
    "                    linear = linear_layer(disc_layers[-1],sizes[i],sizes[i+1])\n",
    "                    disc_layers.append(self.activation_fn(linear))\n",
    "            with tf.variable_scope('class-layer'):\n",
    "                linear = linear_layer(disc_layers[-1],sizes[-1], self.input_space)\n",
    "                disc_layers.append(linear)\n",
    "            return disc_layers\n",
    "        \n",
    "        def train(self):\n",
    "            sess = self.sess\n",
    "            test_x = mnist.test.images\n",
    "            gloss = 0.69\n",
    "            \n",
    "            for i in range(10000):\n",
    "                batch_x,_ = mnist.train.next_batch(self.batch_size)\n",
    "                if gloss > np.log(self.p):\n",
    "                    gloss,_ = sess.run([self.enc_loss,self.train_geenrator],feed_dict={self.input_x:batch_x})\n",
    "                else:\n",
    "                    batch_z = sample_prior(scale=1.0,size=(len(batch_x),self.latent_space))\n",
    "                    gloss,_ = sess.run([self.enc_loss,self.train_discriminator],feed_dict={self.input_x:batch_x,\n",
    "                                                                                          self.z_tensor:batch_z})\n",
    "                if i % 100 == 0:\n",
    "                    gtd = aae.sess.run(aae.generated,feed_dict={aae.z_tensor: sample_prior(size=(4,10))})\n",
    "                    plot_mnist(gtd.reshape([4,28,28]),[1,4])\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            self._encoder()\n",
    "        self.encoded = self.encoder_layers[-1]\n",
    "        \n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            self.decoder_layers = self._decoder(self.encoded)\n",
    "            self.decoded = self.decoder_layers[-1]\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            self.generator_layers = self._decoder(self.z_tensor)\n",
    "            self.generated = tf.nn.sigmoid(self.generator_layers[-1],name=\"generated\")\n",
    "            \n",
    "        sizes = [64,64,1]\n",
    "        with tf.variable_scope(\"discriminator\"):\n",
    "            self.disc_layers_neg = self._discriminator(self.encoded, sizes)\n",
    "            self.disc_neg = self.disc_layers_neg[-1]\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            self.disc_layers_pos = self.self._discriminator(self.z_tensor,sizes)\n",
    "            self.disc_pos = self.disc_layers_pos[-1]\n",
    "            \n",
    "        self.pos_loss = tf.nn.relu(self.disc_pos) - self.disc_pos + tf.log(1.0+tf.exp(-tf.abs(self.disc_pos)))\n",
    "        self.neg_loss = tf.nn.relu(self.disc_neg) + tf.log(1.0 + tf.exp(-tf.abs(self.disc_neg)))\n",
    "        self.disc_loss = tf.reduce_mean(tf.add(self.pos_loss,self.neg_loss))\n",
    "        self.enc_loss = tf.reduce_mean(tf.substract(self.neg_loss,self.disc_neg))\n",
    "        batch_logloss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.decoded,\n",
    "                                                                              labels=self.input_x),1)\n",
    "        self.ae_loss = tf.reduce_mean(batch_logloss)\n",
    "        disc_ws = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='discriminator')\n",
    "        ae_ws = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='encoder') +\\\n",
    "                tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='decoder')\n",
    "        self.l2_loss = tf.multiply(tf.reduce_sum([tf.nn.l2_loss(ws) for ws in ae_ws]), l2_lambda)\n",
    "        \n",
    "        self.gen_loss = tf.add(tf.add(self.enc_loss,self.ae_loss),self,l2_loss)\n",
    "        with tf.variable_scope('optimizers'):\n",
    "            self.train_discriminator = tf.train.RMSPropOptimizer(self.leanring_rate).minimize(self.disc_loss,\n",
    "                                                                                             var_list=disc_ws)\n",
    "            self.train_generator = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.gen_loss,\n",
    "                                                                                         var_list = ae_ws)\n",
    "        self.sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AAE' object has no attribute '_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-52d7bebe0d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-77dba6d4478a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, input_space, latent_space, p, middle_layers, activation_fn, learning_rate, l2_lambda, initializer_fn)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AAE' object has no attribute '_encoder'"
     ]
    }
   ],
   "source": [
    "aae = AAE()\n",
    "aae.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5+\\\n",
    "    4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
